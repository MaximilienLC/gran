{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022/11/16 19:19:14 INFO mlflow.tracking.fluent: Experiment with name 'mnist' does not exist. Creating a new experiment.\n",
      "2022-11-16 19:19:16,427\tINFO worker.py:1528 -- Started a local Ray instance.\n",
      "2022-11-16 19:19:17,621\tWARNING function_trainable.py:586 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-11-16 19:20:47</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:29.48        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.9/15.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/8.33 GiB heap, 0.0/4.16 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  layer_1</th><th style=\"text-align: right;\">  layer_2</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_mnist_tune_909a4_00000</td><td>TERMINATED</td><td>192.168.2.11:31501</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       32</td><td style=\"text-align: right;\">      128</td><td style=\"text-align: right;\">0.00279034 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         60.399 </td><td style=\"text-align: right;\">0.13781 </td><td style=\"text-align: right;\">0.959192</td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00001</td><td>TERMINATED</td><td>192.168.2.11:31533</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       64</td><td style=\"text-align: right;\">      128</td><td style=\"text-align: right;\">0.00135698 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         61.412 </td><td style=\"text-align: right;\">0.126337</td><td style=\"text-align: right;\">0.9626  </td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00002</td><td>TERMINATED</td><td>192.168.2.11:31535</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       64</td><td style=\"text-align: right;\">       64</td><td style=\"text-align: right;\">0.000800728</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         60.0352</td><td style=\"text-align: right;\">0.170907</td><td style=\"text-align: right;\">0.949801</td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00003</td><td>TERMINATED</td><td>192.168.2.11:31537</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      128</td><td style=\"text-align: right;\">      128</td><td style=\"text-align: right;\">0.0107236  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         48.4096</td><td style=\"text-align: right;\">0.153693</td><td style=\"text-align: right;\">0.957807</td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00004</td><td>TERMINATED</td><td>192.168.2.11:31539</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      128</td><td style=\"text-align: right;\">      256</td><td style=\"text-align: right;\">0.00408494 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         49.4064</td><td style=\"text-align: right;\">0.120173</td><td style=\"text-align: right;\">0.967559</td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00005</td><td>TERMINATED</td><td>192.168.2.11:31541</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       64</td><td style=\"text-align: right;\">       64</td><td style=\"text-align: right;\">0.0723153  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         60.1377</td><td style=\"text-align: right;\">0.583836</td><td style=\"text-align: right;\">0.852892</td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00006</td><td>TERMINATED</td><td>192.168.2.11:31543</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      128</td><td style=\"text-align: right;\">      128</td><td style=\"text-align: right;\">0.000268052</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         48.976 </td><td style=\"text-align: right;\">0.236286</td><td style=\"text-align: right;\">0.931322</td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00007</td><td>TERMINATED</td><td>192.168.2.11:31547</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">       32</td><td style=\"text-align: right;\">      256</td><td style=\"text-align: right;\">0.0133476  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         71.8085</td><td style=\"text-align: right;\">0.255765</td><td style=\"text-align: right;\">0.931833</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m 2022/11/16 19:19:21 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m The git executable must be specified in one of the following ways:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m     - be included in your $PATH\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m     - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m     - explicitly set via git.refresh()\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m All git commands will error until this is rectified.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m This initial warning can be silenced or aggravated in the future by setting the\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m     - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m     - warn|w|warning|1: for a printed warning\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m     - error|e|raise|r|2: for a raised exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m Example:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m     export GIT_PYTHON_REFRESH=quiet\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m Missing logger folder: /root/ray_results/mnist/train_mnist_tune_909a4_00000_0_batch_size=64,layer_1=32,layer_2=128,lr=0.0028_2022-11-16_19-19-17/lightning_logs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:88: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:107: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   | Name     | Type     | Params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m 0 | layer_1  | Linear   | 25.1 K\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m 1 | layer_2  | Linear   | 4.2 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m 2 | layer_3  | Linear   | 1.3 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m 3 | accuracy | Accuracy | 0     \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m 30.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m 30.6 K    Total params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m 0.123     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31501)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m 2022/11/16 19:19:28 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m The git executable must be specified in one of the following ways:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m     - be included in your $PATH\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m     - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m     - explicitly set via git.refresh()\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m All git commands will error until this is rectified.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m This initial warning can be silenced or aggravated in the future by setting the\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m     - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m     - warn|w|warning|1: for a printed warning\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m     - error|e|raise|r|2: for a raised exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m Example:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m     export GIT_PYTHON_REFRESH=quiet\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31543)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m Missing logger folder: /root/ray_results/mnist/train_mnist_tune_909a4_00006_6_batch_size=128,layer_1=128,layer_2=128,lr=0.0003_2022-11-16_19-19-22/lightning_logs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:88: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:107: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m 2022/11/16 19:19:29 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m The git executable must be specified in one of the following ways:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m     - be included in your $PATH\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m     - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m     - explicitly set via git.refresh()\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m All git commands will error until this is rectified.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m This initial warning can be silenced or aggravated in the future by setting the\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m     - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m     - warn|w|warning|1: for a printed warning\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m     - error|e|raise|r|2: for a raised exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m Example:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m     export GIT_PYTHON_REFRESH=quiet\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31539)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m 2022/11/16 19:19:29 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m The git executable must be specified in one of the following ways:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m     - be included in your $PATH\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m     - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m     - explicitly set via git.refresh()\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m All git commands will error until this is rectified.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m This initial warning can be silenced or aggravated in the future by setting the\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m     - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m     - warn|w|warning|1: for a printed warning\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m     - error|e|raise|r|2: for a raised exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m Example:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m     export GIT_PYTHON_REFRESH=quiet\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31537)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m 2022/11/16 19:19:29 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m The git executable must be specified in one of the following ways:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m     - be included in your $PATH\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m     - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m     - explicitly set via git.refresh()\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m All git commands will error until this is rectified.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m This initial warning can be silenced or aggravated in the future by setting the\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m     - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m     - warn|w|warning|1: for a printed warning\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m     - error|e|raise|r|2: for a raised exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m Example:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m     export GIT_PYTHON_REFRESH=quiet\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31533)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m 2022/11/16 19:19:29 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m The git executable must be specified in one of the following ways:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m     - be included in your $PATH\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m     - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m     - explicitly set via git.refresh()\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m All git commands will error until this is rectified.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m This initial warning can be silenced or aggravated in the future by setting the\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m     - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m     - warn|w|warning|1: for a printed warning\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m     - error|e|raise|r|2: for a raised exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m Example:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m     export GIT_PYTHON_REFRESH=quiet\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31535)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   | Name     | Type     | Params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m 0 | layer_1  | Linear   | 100 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m 1 | layer_2  | Linear   | 16.5 K\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m 2 | layer_3  | Linear   | 1.3 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m 3 | accuracy | Accuracy | 0     \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m 118 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m 118 K     Total params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m 0.473     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m Missing logger folder: /root/ray_results/mnist/train_mnist_tune_909a4_00004_4_batch_size=128,layer_1=128,layer_2=256,lr=0.0041_2022-11-16_19-19-22/lightning_logs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:88: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:107: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m Missing logger folder: /root/ray_results/mnist/train_mnist_tune_909a4_00002_2_batch_size=64,layer_1=64,layer_2=64,lr=0.0008_2022-11-16_19-19-22/lightning_logs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:88: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:107: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m 2022/11/16 19:19:29 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m The git executable must be specified in one of the following ways:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m     - be included in your $PATH\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m     - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m     - explicitly set via git.refresh()\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m All git commands will error until this is rectified.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m This initial warning can be silenced or aggravated in the future by setting the\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m     - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m     - warn|w|warning|1: for a printed warning\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m     - error|e|raise|r|2: for a raised exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m Example:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m     export GIT_PYTHON_REFRESH=quiet\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31541)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m Missing logger folder: /root/ray_results/mnist/train_mnist_tune_909a4_00001_1_batch_size=64,layer_1=64,layer_2=128,lr=0.0014_2022-11-16_19-19-22/lightning_logs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:88: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:107: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31543)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   | Name     | Type     | Params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m 0 | layer_1  | Linear   | 100 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m 1 | layer_2  | Linear   | 33.0 K\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m 2 | layer_3  | Linear   | 2.6 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m 3 | accuracy | Accuracy | 0     \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m 136 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m 136 K     Total params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m 0.544     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m Missing logger folder: /root/ray_results/mnist/train_mnist_tune_909a4_00003_3_batch_size=128,layer_1=128,layer_2=128,lr=0.0107_2022-11-16_19-19-22/lightning_logs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:88: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:107: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   | Name     | Type     | Params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m 0 | layer_1  | Linear   | 50.2 K\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m 1 | layer_2  | Linear   | 8.3 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m 2 | layer_3  | Linear   | 1.3 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m 3 | accuracy | Accuracy | 0     \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m 59.9 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m 59.9 K    Total params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m 0.239     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   | Name     | Type     | Params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m 0 | layer_1  | Linear   | 50.2 K\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m 1 | layer_2  | Linear   | 4.2 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m 2 | layer_3  | Linear   | 650   \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m 3 | accuracy | Accuracy | 0     \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m 55.1 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m 55.1 K    Total params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m 0.220     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m 2022/11/16 19:19:29 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m The git executable must be specified in one of the following ways:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m     - be included in your $PATH\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m     - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m     - explicitly set via git.refresh()\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m All git commands will error until this is rectified.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m This initial warning can be silenced or aggravated in the future by setting the\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m     - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m     - warn|w|warning|1: for a printed warning\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m     - error|e|raise|r|2: for a raised exception\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m Example:\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m     export GIT_PYTHON_REFRESH=quiet\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=31547)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   | Name     | Type     | Params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m 0 | layer_1  | Linear   | 100 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m 1 | layer_2  | Linear   | 16.5 K\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m 2 | layer_3  | Linear   | 1.3 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m 3 | accuracy | Accuracy | 0     \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m 118 K     Trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m 118 K     Total params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m 0.473     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31535)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m Missing logger folder: /root/ray_results/mnist/train_mnist_tune_909a4_00005_5_batch_size=64,layer_1=64,layer_2=64,lr=0.0723_2022-11-16_19-19-22/lightning_logs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:88: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:107: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m Missing logger folder: /root/ray_results/mnist/train_mnist_tune_909a4_00007_7_batch_size=32,layer_1=32,layer_2=256,lr=0.0133_2022-11-16_19-19-22/lightning_logs\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:88: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/datamodule.py:107: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31539)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31533)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   | Name     | Type     | Params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m 0 | layer_1  | Linear   | 50.2 K\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m 1 | layer_2  | Linear   | 4.2 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m 2 | layer_3  | Linear   | 650   \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m 3 | accuracy | Accuracy | 0     \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m 55.1 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m 55.1 K    Total params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m 0.220     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31537)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   | Name     | Type     | Params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m 0 | layer_1  | Linear   | 25.1 K\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m 1 | layer_2  | Linear   | 8.4 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m 2 | layer_3  | Linear   | 2.6 K \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m 3 | accuracy | Accuracy | 0     \n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m --------------------------------------\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m 36.1 K    Trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m 36.1 K    Total params\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m 0.145     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31541)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(train_mnist_tune pid=31547)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th style=\"text-align: right;\">     acc</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>experiment_tag                                    </th><th>hostname        </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">    loss</th><th>node_ip     </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_mnist_tune_909a4_00000</td><td style=\"text-align: right;\">0.959192</td><td>2022-11-16_19-20-22</td><td>True  </td><td>                </td><td>7e66b62dcdc44db0869e6f2914d12da4</td><td>0_batch_size=64,layer_1=32,layer_2=128,lr=0.0028  </td><td>MaxUbuntuDesktop</td><td style=\"text-align: right;\">                         3</td><td style=\"text-align: right;\">0.13781 </td><td>192.168.2.11</td><td style=\"text-align: right;\">31501</td><td style=\"text-align: right;\">             60.399 </td><td style=\"text-align: right;\">           19.8846</td><td style=\"text-align: right;\">       60.399 </td><td style=\"text-align: right;\"> 1668626422</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>909a4_00000</td><td style=\"text-align: right;\">    0.0162296</td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00001</td><td style=\"text-align: right;\">0.9626  </td><td>2022-11-16_19-20-30</td><td>True  </td><td>                </td><td>791319d8c9654459a713337c4195f1c7</td><td>1_batch_size=64,layer_1=64,layer_2=128,lr=0.0014  </td><td>MaxUbuntuDesktop</td><td style=\"text-align: right;\">                         3</td><td style=\"text-align: right;\">0.126337</td><td>192.168.2.11</td><td style=\"text-align: right;\">31533</td><td style=\"text-align: right;\">             61.412 </td><td style=\"text-align: right;\">           19.2996</td><td style=\"text-align: right;\">       61.412 </td><td style=\"text-align: right;\"> 1668626430</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>909a4_00001</td><td style=\"text-align: right;\">    0.0517948</td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00002</td><td style=\"text-align: right;\">0.949801</td><td>2022-11-16_19-20-29</td><td>True  </td><td>                </td><td>8d3fd665b6d64746bad46aca411f4bcc</td><td>2_batch_size=64,layer_1=64,layer_2=64,lr=0.0008   </td><td>MaxUbuntuDesktop</td><td style=\"text-align: right;\">                         3</td><td style=\"text-align: right;\">0.170907</td><td>192.168.2.11</td><td style=\"text-align: right;\">31535</td><td style=\"text-align: right;\">             60.0352</td><td style=\"text-align: right;\">           18.8297</td><td style=\"text-align: right;\">       60.0352</td><td style=\"text-align: right;\"> 1668626429</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>909a4_00002</td><td style=\"text-align: right;\">    0.0255485</td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00003</td><td style=\"text-align: right;\">0.957807</td><td>2022-11-16_19-20-17</td><td>True  </td><td>                </td><td>b4f859cb9f6441b1864e7faadb9fdf89</td><td>3_batch_size=128,layer_1=128,layer_2=128,lr=0.0107</td><td>MaxUbuntuDesktop</td><td style=\"text-align: right;\">                         3</td><td style=\"text-align: right;\">0.153693</td><td>192.168.2.11</td><td style=\"text-align: right;\">31537</td><td style=\"text-align: right;\">             48.4096</td><td style=\"text-align: right;\">           16.3549</td><td style=\"text-align: right;\">       48.4096</td><td style=\"text-align: right;\"> 1668626417</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>909a4_00003</td><td style=\"text-align: right;\">    0.047677 </td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00004</td><td style=\"text-align: right;\">0.967559</td><td>2022-11-16_19-20-18</td><td>True  </td><td>                </td><td>a0f11e4dce514981811817f664bc8ef0</td><td>4_batch_size=128,layer_1=128,layer_2=256,lr=0.0041</td><td>MaxUbuntuDesktop</td><td style=\"text-align: right;\">                         3</td><td style=\"text-align: right;\">0.120173</td><td>192.168.2.11</td><td style=\"text-align: right;\">31539</td><td style=\"text-align: right;\">             49.4064</td><td style=\"text-align: right;\">           17.0806</td><td style=\"text-align: right;\">       49.4064</td><td style=\"text-align: right;\"> 1668626418</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>909a4_00004</td><td style=\"text-align: right;\">    0.0365021</td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00005</td><td style=\"text-align: right;\">0.852892</td><td>2022-11-16_19-20-29</td><td>True  </td><td>                </td><td>41dcdc392252400d9a5c699569cb714b</td><td>5_batch_size=64,layer_1=64,layer_2=64,lr=0.0723   </td><td>MaxUbuntuDesktop</td><td style=\"text-align: right;\">                         3</td><td style=\"text-align: right;\">0.583836</td><td>192.168.2.11</td><td style=\"text-align: right;\">31541</td><td style=\"text-align: right;\">             60.1377</td><td style=\"text-align: right;\">           18.7456</td><td style=\"text-align: right;\">       60.1377</td><td style=\"text-align: right;\"> 1668626429</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>909a4_00005</td><td style=\"text-align: right;\">    0.0587533</td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00006</td><td style=\"text-align: right;\">0.931322</td><td>2022-11-16_19-20-17</td><td>True  </td><td>                </td><td>3ba2851e7bbb47d1adcacbf56ce12f4a</td><td>6_batch_size=128,layer_1=128,layer_2=128,lr=0.0003</td><td>MaxUbuntuDesktop</td><td style=\"text-align: right;\">                         3</td><td style=\"text-align: right;\">0.236286</td><td>192.168.2.11</td><td style=\"text-align: right;\">31543</td><td style=\"text-align: right;\">             48.976 </td><td style=\"text-align: right;\">           16.51  </td><td style=\"text-align: right;\">       48.976 </td><td style=\"text-align: right;\"> 1668626417</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>909a4_00006</td><td style=\"text-align: right;\">    0.0252337</td></tr>\n",
       "<tr><td>train_mnist_tune_909a4_00007</td><td style=\"text-align: right;\">0.931833</td><td>2022-11-16_19-20-41</td><td>True  </td><td>                </td><td>b93b7c2f418043fbafdd3ccf363ac169</td><td>7_batch_size=32,layer_1=32,layer_2=256,lr=0.0133  </td><td>MaxUbuntuDesktop</td><td style=\"text-align: right;\">                         3</td><td style=\"text-align: right;\">0.255765</td><td>192.168.2.11</td><td style=\"text-align: right;\">31547</td><td style=\"text-align: right;\">             71.8085</td><td style=\"text-align: right;\">           13.0043</td><td style=\"text-align: right;\">       71.8085</td><td style=\"text-align: right;\"> 1668626441</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>909a4_00007</td><td style=\"text-align: right;\">    0.0456395</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 19:20:47,242\tINFO tune.py:777 -- Total run time: 89.62 seconds (89.47 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'layer_1': 128, 'layer_2': 256, 'lr': 0.004084936760385664, 'batch_size': 128, 'mlflow': {'experiment_name': 'mnist', 'tracking_uri': 'file:///mlruns'}, 'data_dir': '/tmp/mnist_data_', 'num_epochs': 3}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pl_bolts.datamodules import MNISTDataModule\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from ray import air, tune\n",
    "from ray.tune.integration.mlflow import mlflow_mixin\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class LightningMNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, config, data_dir=None):\n",
    "        super(LightningMNISTClassifier, self).__init__()\n",
    "\n",
    "        self.data_dir = data_dir or os.getcwd()\n",
    "        self.lr = config[\"lr\"]\n",
    "        layer_1, layer_2 = config[\"layer_1\"], config[\"layer_2\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = torch.nn.Linear(28 * 28, layer_1)\n",
    "        self.layer_2 = torch.nn.Linear(layer_1, layer_2)\n",
    "        self.layer_3 = torch.nn.Linear(layer_2, 10)\n",
    "        self.accuracy = Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        self.log(\"ptl/train_accuracy\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss)\n",
    "        self.log(\"ptl/val_accuracy\", avg_acc)\n",
    "\n",
    "@mlflow_mixin\n",
    "def train_mnist_tune(config, data_dir=None, num_epochs=10, num_gpus=0):\n",
    "    model = LightningMNISTClassifier(config, data_dir)\n",
    "    dm = MNISTDataModule(\n",
    "        data_dir=data_dir, num_workers=1, batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "    metrics = {\"loss\": \"ptl/val_loss\", \"acc\": \"ptl/val_accuracy\"}\n",
    "    mlflow.pytorch.autolog()\n",
    "    mlflow.log_param(\"layer_1\", config[\"layer_1\"])\n",
    "    mlflow.log_param(\"layer_2\", config[\"layer_2\"],)\n",
    "    mlflow.log_param(\"batch_size\", config[\"batch_size\"],)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        gpus=num_gpus,\n",
    "        progress_bar_refresh_rate=0,\n",
    "        callbacks=[TuneReportCallback(metrics, on=\"validation_end\")],\n",
    "    )\n",
    "    trainer.fit(model, dm)\n",
    "\n",
    "\n",
    "def tune_mnist(\n",
    "    num_samples=10,\n",
    "    num_epochs=10,\n",
    "    gpus_per_trial=0,\n",
    "    tracking_uri=None,\n",
    "    experiment_name=\"mnist\",\n",
    "):\n",
    "    data_dir = os.path.join(tempfile.gettempdir(), \"mnist_data_\")\n",
    "    # Download data\n",
    "    MNISTDataModule(data_dir=data_dir).prepare_data()\n",
    "\n",
    "    # Set the MLflow experiment, or create it if it does not exist.\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    config = {\n",
    "        \"layer_1\": tune.choice([32, 64, 128]),\n",
    "        \"layer_2\": tune.choice([64, 128, 256]),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([32, 64, 128]),\n",
    "        \"mlflow\": {\n",
    "            \"experiment_name\": experiment_name,\n",
    "            \"tracking_uri\": mlflow.get_tracking_uri(),\n",
    "        },\n",
    "        \"data_dir\": os.path.join(tempfile.gettempdir(), \"mnist_data_\"),\n",
    "        \"num_epochs\": num_epochs,\n",
    "    }\n",
    "\n",
    "    trainable = tune.with_parameters(\n",
    "        train_mnist_tune,\n",
    "        data_dir=data_dir,\n",
    "        num_epochs=num_epochs,\n",
    "        num_gpus=gpus_per_trial,\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(trainable, resources={\"cpu\": 1, \"gpu\": gpus_per_trial}),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        run_config=air.RunConfig(\n",
    "            name=\"mnist\",\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", results.get_best_result().config)\n",
    "\n",
    "tune_mnist(num_samples=8, num_epochs=3, gpus_per_trial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
